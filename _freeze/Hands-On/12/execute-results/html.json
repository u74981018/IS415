{
  "hash": "535723c71e14aeb0abfb6137705ac384",
  "result": {
    "engine": "knitr",
    "markdown": "# **13  Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method**\n\n\n---\nauthor: \"Lucas Vial\"\ndate: \"Nov 2024\"\ndate-modified: \"Date\"\nexecute: \n  eval: false\n  echo: true\n  freeze: true\n  warning: false\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n## **13.1 Overview**\n\n**Geographically weighted regression (GWR)** is a spatial statistical\ntechnique that takes non-stationary variables into consideration (e.g.,\nclimate; demographic factors; physical environment characteristics) and\nmodels the local relationships between these independent variables and\nan outcome of interest (also known as dependent variable). In this\nhands-on exercise, you will learn how to build [hedonic\npricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models\nby using GWR methods. The dependent variable is the resale prices of\ncondominium in 2015. The independent variables are divided into either\nstructural and locational.\n\n## **13.2 The Data**\n\nTwo data sets will be used in this model building exercise, they are:\n\n-   URA Master Plan subzone boundary in shapefile format\n    (i.e. *MP14_SUBZONE_WEB_PL*)\n\n-   condo_resale_2015 in csv format (i.e. *condo_resale_2015.csv*\n\n## **13.3 Getting Started**\n\nBefore we get started, it is important for us to install the necessary R\npackages into R and launch these R packages into R environment.\n\nThe R packages needed for this exercise are as follows:\n\n-   R package for building OLS and performing diagnostics tests\n\n    -   [**olsrr**](https://olsrr.rsquaredacademy.com/)\n\n-   R package for calibrating geographical weighted family of models\n\n    -   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html)\n\n-   R package for multivariate data visualisation and analysis\n\n    -   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)\n\n-   Spatial data handling\n\n    -   **sf**\n\n-   Attribute data handling\n\n    -   **tidyverse**, especially **readr**, **ggplot2** and **dplyr**\n\n-   Choropleth mapping\n\n    -   **tmap**\n\nThe code chunks below installs and launches these R packages into R\nenvironment\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse)\n```\n:::\n\n\n## **13.4 A short note about GWmodel**\n\n[**GWmodel**](https://www.jstatsoft.org/article/view/v063i17) package\nprovides a collection of localised spatial statistical methods, namely:\nGW summary statistics, GW principal components analysis, GW discriminant\nanalysis and various forms of GW regression; some of which are provided\nin basic and robust (outlier resistant) forms. Commonly, outputs or\nparameters of the GWmodel are mapped to provide a useful exploratory\ntool, which can often precede (and direct) a more traditional or\nsophisticated statistical analysis.\n\n## **13.5 Geospatial Data Wrangling**\n\n### **13.5.1 Importing geospatial data**\n\nThe geospatial data used in this hands-on exercise is called\nMP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile\nconsists of URA Master Plan 2014’s planning subzone boundaries. Polygon\nfeatures are used to represent these geographic boundaries. The GIS data\nis in svy21 projected coordinates systems.\n\nThe code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by\nusing `st_read()` of **sf** packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz = st_read(dsn = \"../inclass/data/2data\", layer = \"MP14_SUBZONE_WEB_PL\")\n```\n:::\n\n\nThe report above shows that the R object used to contain the imported\nMP14_SUBZONE_WEB_PL shapefile is called *mpsz* and it is a simple\nfeature object. The geometry type is *multipolygon*. it is also\nimportant to note that mpsz simple feature object does not have EPSG\ninformation.\n\n### **13.5.2 Updating CRS information**\n\nThe code chunk below updates the newly imported *mpsz* with the correct\nESPG code (i.e. 3414)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_svy21 <- st_transform(mpsz, 3414)\n```\n:::\n\n\nAfter transforming the projection metadata, you can varify the\nprojection of the newly transformed *mpsz_svy21* by using `st_crs()` of\n**sf** package.\n\nThe code chunk below will be used to varify the newly transformed\n*mpsz_svy21*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(mpsz_svy21)\n```\n:::\n\n\nNotice that the EPSG: is indicated as *3414* now.\n\nNext, you will reveal the extent of *mpsz_svy21* by using `st_bbox()` of\nsf package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_bbox(mpsz_svy21) #view extent\n```\n:::\n\n\n## **13.6 Aspatial Data Wrangling**\n\n### **13.6.1 Importing the aspatial data**\n\nThe *condo_resale_2015* is in csv file format. The codes chunk below\nuses `read_csv()` function of **readr** package to import\n*condo_resale_2015* into R as a tibble data frame called *condo_resale*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale = read_csv(\"data/Condo_resale_2015.csv\")\n```\n:::\n\n\nAfter importing the data file into R, it is important for us to examine\nif the data file has been imported correctly.\n\nThe codes chunks below uses `glimpse()` to display the data structure of\nwill do the job.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(condo_resale)\n```\n:::\n\n\n### **13.6.2 Converting aspatial data frame into a sf object**\n\nCurrently, the *condo_resale* tibble data frame is aspatial. We will\nconvert it to a **sf** object. The code chunk below converts\ncondo_resale data frame into a simple feature data frame by using\n`st_as_sf()` of **sf** packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n```\n:::\n\n\nNotice that `st_transform()` of **sf** package is used to convert the\ncoordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\n\nNext, `head()` is used to list the content of *condo_resale.sf* object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(condo_resale.sf)\n```\n:::\n\n\nNotice that the output is in point feature data frame.\n\n## **13.7 Exploratory Data Analysis (EDA)**\n\nIn the section, you will learn how to use statistical graphics functions\nof **ggplot2** package to perform EDA.\n\n### **13.7.1 EDA using statistical graphics**\n\nWe can plot the distribution of *SELLING_PRICE* by using appropriate\nExploratory Data Analysis (EDA) as shown in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n```\n:::\n\n\nThe figure above reveals a right skewed distribution. This means that\nmore condominium units were transacted at relative lower prices.\n\nStatistically, the skewed dsitribution can be normalised by using log\ntransformation. The code chunk below is used to derive a new variable\ncalled *LOG_SELLING_PRICE* by using a log transformation on the variable\n*SELLING_PRICE*. It is performed using `mutate()` of **dplyr** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n```\n:::\n\n\nNow, you can plot the *LOG_SELLING_PRICE* using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n```\n:::\n\n\nNotice that the distribution is relatively less skewed after the\ntransformation.\n\n### **13.7.2 Multiple Histogram Plots distribution of variables**\n\nIn this section, you will learn how to draw a small multiple histograms\n(also known as trellis plot) by using `ggarrange()` of\n[**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/index.html)\npackage.\n\nThe code chunk below is used to create 12 histograms. Then,\n`ggarrange()` is used to organised these histogram into a 3 columns by 4\nrows small multiple plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n```\n:::\n\n\n### **13.7.3 Drawing Statistical Point Map**\n\nLastly, we want to reveal the geospatial distribution condominium resale\nprices in Singapore. The map will be prepared by using **tmap** package.\n\nFirst, we will turn on the interactive mode of tmap by using the code\nchunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\n```\n:::\n\n\nNext, the code chunks below is used to create an interactive point\nsymbol map.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14)) + \n  tmap_options(check.and.fix = TRUE)\n```\n:::\n\n\nNotice that\n[`tm_dots()`](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_symbols)\nis used instead of `tm_bubbles()`.\n\n`set.zoom.limits` argument of `tm_view()` sets the minimum and maximum\nzoom level to 11 and 14 respectively.\n\nBefore moving on to the next section, the code below will be used to\nturn R display into `plot` mode.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n```\n:::\n\n\n## **13.8 Hedonic Pricing Modelling in R**\n\nIn this section, you will learn how to building hedonic pricing models\nfor condominium resale units using\n[`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/lm)\nof R base.\n\n### **13.8.1 Simple Linear Regression Method**\n\nFirst, we will build a simple linear regression model by using\n*SELLING_PRICE* as the dependent variable and *AREA_SQM* as the\nindependent variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n```\n:::\n\n\n`lm()` returns an object of class “lm” or for multiple responses of\nclass c(“mlm”, “lm”).\n\nThe functions `summary()` and `anova()` can be used to obtain and print\na summary and analysis of variance table of the results. The generic\naccessor functions coefficients, effects, fitted.values and residuals\nextract various useful features of the value returned by `lm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(condo.slr)\n```\n:::\n\n\nThe output report reveals that the SELLING_PRICE can be explained by\nusing the formula:\n\n```         \n      *y = -258121.1 + 14719x1*\n```\n\nThe R-squared of 0.4518 reveals that the simple regression model built\nis able to explain about 45% of the resale prices.\n\nSince p-value is much smaller than 0.0001, we will reject the null\nhypothesis that mean is a good estimator of SELLING_PRICE. This will\nallow us to infer that simple linear regression model above is a good\nestimator of *SELLING_PRICE*.\n\nThe **Coefficients:** section of the report reveals that the p-values of\nboth the estimates of the Intercept and ARA_SQM are smaller than 0.001.\nIn view of this, the null hypothesis of the B0 and B1 are equal to 0\nwill be rejected. As a results, we will be able to infer that the B0 and\nB1 are good parameter estimates.\n\nTo visualise the best fit curve on a scatterplot, we can incorporate\n`lm()` as a method function in ggplot’s geometry as shown in the code\nchunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n```\n:::\n\n\nFigure above reveals that there are a few statistical outliers with\nrelatively high selling prices.\n\n### **13.8.2 Multiple Linear Regression Method**\n\n#### 13.8.2.1 Visualising the relationships of the independent variables\n\nBefore building a multiple regression model, it is important to ensure\nthat the indepdent variables used are not highly correlated to each\nother. If these highly correlated independent variables are used in\nbuilding a regression model by mistake, the quality of the model will be\ncompromised. This phenomenon is known as **multicollinearity** in\nstatistics.\n\nCorrelation matrix is commonly used to visualise the relationships\nbetween the independent variables. Beside the `pairs()` of R, there are\nmany packages support the display of a correlation matrix. In this\nsection, the\n[**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)\npackage will be used.\n\nThe code chunk below is used to plot a scatterplot matrix of the\nrelationship between the independent variables in *condo_resale*\ndata.frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n```\n:::\n\n\nMatrix reorder is very important for mining the hiden structure and\npatter in the matrix. There are four methods in corrplot (parameter\norder), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk\nabove, AOE order is used. It orders the variables by using the *angular\norder of the eigenvectors* method suggested by [Michael\nFriendly](https://www.datavis.ca/papers/corrgram.pdf).\n\nFrom the scatterplot matrix, it is clear that ***Freehold*** is highly\ncorrelated to ***LEASE_99YEAR***. In view of this, it is wiser to only\ninclude either one of them in the subsequent model building. As a\nresult, ***LEASE_99YEAR*** is excluded in the subsequent model building.\n\n### **13.8.3 Building a hedonic pricing model using multiple linear regression method**\n\nThe code chunk below using `lm()` to calibrate the multiple linear\nregression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n```\n:::\n\n\n### **13.8.4 Preparing Publication Quality Table: olsrr method**\n\nWith reference to the report above, it is clear that not all the\nindependent variables are statistically significant. We will revised the\nmodel by removing those variables which are not statistically\nsignificant.\n\nNow, we are ready to calibrate the revised model by using the code chunk\nbelow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n```\n:::\n\n\n### **13.8.5 Preparing Publication Quality Table: gtsummary method**\n\nThe [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/index.html)\npackage provides an elegant and flexible way to create publication-ready\nsummary tables in R.\n\nIn the code chunk below,\n[`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html)\nis used to create a well formatted regression report.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#pacman::p_load(gtsummary)\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n```\n:::\n\n\nWith gtsummary package, model statistics can be included in the report\nby either appending them to the report table by using\n[`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html)\nor adding as a table source note by using\n[`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html)\nas shown in the code chunk below.ing\n[`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html)\nas shown in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n```\n:::\n\n\nFor more customisation options, refer to [Tutorial:\ntbl_regression](https://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)\n\n#### 13.8.5.1 Checking for multicolinearity\n\nIn this section, we would like to introduce you a fantastic R package\nspecially programmed for performing OLS regression. It is called\n[**olsrr**](https://olsrr.rsquaredacademy.com/). It provides a\ncollection of very useful methods for building better multiple linear\nregression models:\n\n-   comprehensive regression output\n\n-   residual diagnostics\n\n-   measures of influence\n\n-   heteroskedasticity tests\n\n-   collinearity diagnostics\n\n-   model fit assessment\n\n-   variable contribution assessment\n\n-   variable selection procedures\n\nIn the code chunk below, the\n[`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html)\nof **olsrr** package is used to test if there are sign of\nmulticollinearity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_vif_tol(condo.mlr1)\n```\n:::\n\n\nSince the VIF of the independent variables are less than 10. We can\nsafely conclude that there are no sign of multicollinearity among the\nindependent variables.\n\n#### 13.8.5.2 Test for Non-Linearity\n\nIn multiple linear regression, it is important for us to test the\nassumption that linearity and additivity of the relationship between\ndependent and independent variables.\n\nIn the code chunk below, the\n[`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html)\nof **olsrr** package is used to perform linearity assumption test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_plot_resid_fit(condo.mlr1)\n```\n:::\n\n\nThe figure above reveals that most of the data poitns are scattered\naround the 0 line, hence we can safely conclude that the relationships\nbetween the dependent variable and independent variables are linear.\n\n#### 13.8.5.3 Test for Normality Assumption\n\nLastly, the code chunk below uses\n[`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html)\nof *olsrr* package to perform normality assumption test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_plot_resid_hist(condo.mlr1)\n```\n:::\n\n\nThe figure reveals that the residual of the multiple linear regression\nmodel (i.e. condo.mlr1) is resemble normal distribution.\n\nIf you prefer formal statistical test methods, the\n[`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html)\nof **olsrr** package can be used as shown in the code chun below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_test_normality(condo.mlr1)\n```\n:::\n\n\nThe summary table above reveals that the p-values of the four tests are\nway smaller than the alpha value of 0.05. Hence we will reject the null\nhypothesis and infer that there is statistical evidence that the\nresidual are not normally distributed.\n\n#### 13.8.5.4 Testing for Spatial Autocorrelation\n\nThe hedonic model we try to build are using geographically referenced\nattributes, hence it is also important for us to visual the residual of\nthe hedonic pricing model.\n\nIn order to perform spatial autocorrelation test, we need to convert\n*condo_resale.sf* from sf data frame into a **SpatialPointsDataFrame**.\n\nFirst, we will export the residual of the hedonic pricing model and save\nit as a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n```\n:::\n\n\nNext, we will join the newly created data frame with *condo_resale.sf*\nobject.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n```\n:::\n\n\nNext, we will convert *condo_resale.res.sf* from simple feature object\ninto a SpatialPointsDataFrame because spdep package can only process sp\nconformed spatial data objects.\n\nThe code chunk below will be used to perform the data conversion\nprocess.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n```\n:::\n\n\nNext, we will use **tmap** package to display the distribution of the\nresiduals on an interactive map.\n\nThe code churn below will turn on the interactive mode of tmap.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n```\n:::\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\n\nTo proof that our observation is indeed true, the Moran’s I test will be\nperformed\n\nFirst, we will compute the distance-based weight matrix by using\n[`dnearneigh()`](https://r-spatial.github.io/spdep/reference/dnearneigh.html)\nfunction of **spdep**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n```\n:::\n\n\nNext,\n[`nb2listw()`](https://r-spatial.github.io/spdep/reference/nb2listw.html)\nof **spdep** packge will be used to convert the output neighbours lists\n(i.e. nb) into a spatial weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_lw <- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n```\n:::\n\n\nNext,\n[`lm.morantest()`](https://r-spatial.github.io/spdep/reference/lm.morantest.html)\nof **spdep** package will be used to perform Moran’s I test for residual\nspatial autocorrelation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.morantest(condo.mlr1, nb_lw)\n```\n:::\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows\nthat it’s p-value is less than 0.00000000000000022 which is less than\nthe alpha value of 0.05. Hence, we will reject the null hypothesis that\nthe residuals are randomly distributed.\n\nSince the Observed Global Moran I = 0.1424418 which is greater than 0,\nwe can infer than the residuals resemble cluster distribution.\n\n## **13.9 Building Hedonic Pricing Models using GWmodel**\n\nIn this section, you are going to learn how to modelling hedonic pricing\nusing both the fixed and adaptive bandwidth schemes\n\n### **13.9.1 Building Fixed Bandwidth GWR Model**\n\n#### 13.9.1.1 Computing fixed bandwith\n\nIn the code chunk below `bw.gwr()` of GWModel package is used to\ndetermine the optimal fixed bandwidth to use in the model. Notice that\nthe argument ***adaptive*** is set to **FALSE** indicates that we are\ninterested to compute the fixed bandwidth.\n\nThere are two possible approaches can be uused to determine the stopping\nrule, they are: CV cross-validation approach and AIC corrected (AICc)\napproach. We define the stopping rule using ***approach*** argeement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n```\n:::\n\n\nThe result shows that the recommended bandwidth is 971.3405 metres.\n(Quiz: Do you know why it is in metre?)\n\n#### 13.9.1.2 GWModel method - fixed bandwith\n\nNow we can use the code chunk below to calibrate the gwr model using\nfixed bandwidth and gaussian kernel.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n```\n:::\n\n\nThe output is saved in a list of class “gwrm”. The code below can be\nused to display the model output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr.fixed\n```\n:::\n\n\nThe report shows that the AICc of the gwr is 42263.61 which is\nsignificantly smaller than the globel multiple linear regression model\nof 42967.1.\n\n### **13.9.2 Building Adaptive Bandwidth GWR Model**\n\nIn this section, we will calibrate the gwr-based hedonic pricing model\nby using adaptive bandwidth approach.\n\n#### 13.9.2.1 Computing the adaptive bandwidth\n\nSimilar to the earlier section, we will first use `bw.gwr()` to\ndetermine the recommended data point to use.\n\nThe code chunk used look very similar to the one used to compute the\nfixed bandwidth except the `adaptive` argument has changed to **TRUE**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n```\n:::\n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n#### 13.9.2.2 Constructing the adaptive bandwidth gwr model\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by\nusing adaptive bandwidth and gaussian kernel as shown in the code chunk\nbelow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n```\n:::\n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22\nwhich is even smaller than the AICc of the fixed distance gwr of\n42263.61.\n\n### **13.9.3 Visualising GWR Output**\n\nIn addition to regression residuals, the output feature class table\nincludes fields for observed and predicted y values, condition number\n(cond), Local R2, residuals, and explanatory variable coefficients and\nstandard errors:\n\n-   Condition Number: this diagnostic evaluates local collinearity. In\n    the presence of strong local collinearity, results become unstable.\n    Results associated with condition numbers larger than 30, may be\n    unreliable.\n\n-   Local R2: these values range between 0.0 and 1.0 and indicate how\n    well the local regression model fits observed y values. Very low\n    values indicate the local model is performing poorly. Mapping the\n    Local R2 values to see where GWR predicts well and where it predicts\n    poorly may provide clues about important variables that may be\n    missing from the regression model.\n\n-   Predicted: these are the estimated (or fitted) y values 3. computed\n    by GWR.\n\n-   Residuals: to obtain the residual values, the fitted y values are\n    subtracted from the observed y values. Standardized residuals have a\n    mean of zero and a standard deviation of 1. A cold-to-hot rendered\n    map of standardized residuals can be produce by using these values.\n\n-   Coefficient Standard Error: these values measure the reliability of\n    each coefficient estimate. Confidence in those estimates are higher\n    when standard errors are small in relation to the actual coefficient\n    values. Large standard errors may indicate problems with local\n    collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or\nSpatialPolygonsDataFrame object integrated with fit.points, GWR\ncoefficient estimates, y value, predicted values, coefficient standard\nerrors and t-values in its “data” slot in an object called **SDF** of\nthe output list.\n\n### **13.9.4 Converting SDF into *sf* data.frame**\n\nTo visualise the fields in **SDF**, we need to first covert it into\n**sf** data.frame by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nglimpse(condo_resale.sf.adaptive)\n```\n:::\n\n\n### **13.9.5 Visualising local R2**\n\nThe code chunks below is used to create an interactive point symbol map.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n```\n:::\n\n\n### **13.9.6 Visualising coefficient estimates**\n\nThe code chunks below is used to create an interactive point symbol map.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n```\n:::\n\n\n#### 13.9.6.1 By URA Plannign Region\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)\n```\n:::\n",
    "supporting": [
      "12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}